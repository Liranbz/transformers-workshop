{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "transformer_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuvalPeleg/transformers-workshop/blob/master/transformer_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8oNdSmas-HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nPFypfk8s2d",
        "colab_type": "code",
        "outputId": "25d83e37-6109-4811-90b3-6c5644a5b5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "TODO:\n",
        "--test code\n",
        "\n",
        "--understand masking\n",
        "\n",
        "--understand positional encoding\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTODO:\\n--test code\\n\\n--understand masking\\n\\n--understand positional encoding\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcUn9MXFtGGh",
        "colab_type": "code",
        "outputId": "dc4126ad-2979-4b30-b51b-2f593abab11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#!pip3 install  https://download.pytorch.org/whl/cu90/torch-1.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install https://download.pytorch.org/whl/cu92/torch-1.3.0%2Bcu92-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchtext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.3.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.3.0%2Bcu92-cp36-cp36m-linux_x86_64.whl (660.8MB)\n",
            "\u001b[K     |████████████████████████████████| 660.8MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.0+cu92) (1.17.3)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.3.0+cu100\n",
            "    Uninstalling torch-1.3.0+cu100:\n",
            "      Successfully uninstalled torch-1.3.0+cu100\n",
            "Successfully installed torch-1.3.0+cu92\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.3.0+cu92)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.9.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd731kPLs-Ha",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on how to train a sequence-to-sequence model\n",
        "that uses the\n",
        "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
        "\n",
        "PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper `Attention is All You\n",
        "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
        "has been proved to be superior in quality for many sequence-to-sequence\n",
        "problems while being more parallelizable. The ``nn.Transformer`` module\n",
        "relies entirely on an attention mechanism (another module recently\n",
        "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
        "between input and output. The ``nn.Transformer`` module is now highly\n",
        "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
        "in this tutorial) can be easily adapted/composed.\n",
        "\n",
        "![](../_static/img/transformer_architecture.jpg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhnCqfnmD23U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFZYpj0Cs-Hb",
        "colab_type": "text"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3CyHrY3s-Hb",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "`nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
        "the sequence. For the language modeling task, any tokens on the future\n",
        "positions should be masked. To have the actual words, the output\n",
        "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
        "layer, which is followed by a log-Softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVKJC4us-Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POEOcztRs-He",
        "colab_type": "text"
      },
      "source": [
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a95BXQF6s-Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heYxh4zks-Hh",
        "colab_type": "text"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Hd6zdIs-Hh",
        "colab_type": "text"
      },
      "source": [
        "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26)\n",
        "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
        "length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "These columns are treated as independent by the model, which means that\n",
        "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
        "efficient batch processing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9BfdGecs-Hi",
        "colab_type": "code",
        "outputId": "e52ada3c-ca11-41f7-802d-686dfaf1e9f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    if hasattr(data, 'examples'):      \n",
        "      text = data.examples[0].text\n",
        "    else:\n",
        "      text = data\n",
        "    data = TEXT.numericalize([text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:01<00:00, 3.04MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2GFrOVzxJZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torchtext.datasets.WikiText2.splits?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68lMjb9Vw5vG",
        "colab_type": "code",
        "outputId": "5a2b2ea8-8113-419c-a42a-e27fb29622f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TEXT.preprocess(\"I like cats\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'like', 'cats']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6pCljjNhKEz",
        "colab_type": "code",
        "outputId": "80760c98-cde2-4ccf-c54c-caffa3a99983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(TEXT.numericalize([['i'], ['valkyria']]).shape)\n",
        "print(TEXT.numericalize([\"i\"]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[79]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f1yYUDlhoqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.numericalize?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zS3y-98iyMf",
        "colab_type": "code",
        "outputId": "f52976f7-c148-4cb1-c634-e21719be4af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_txt.examples[0].text[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', '<eos>', ' ', '=', 'valkyria']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-BhtUuse2lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYUkaejucTPP",
        "colab_type": "code",
        "outputId": "e86fca50-433e-4f2f-faec-55d2f6eb4b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batchify(\"Paris is the capital of\", 1024)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([], device='cuda:0', size=(0, 1024), dtype=torch.int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZUWYayS1vvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_text_lst = list(val_txt.examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_w1ayls-Hk",
        "colab_type": "text"
      },
      "source": [
        "Functions to generate input and target sequence\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d2yNwER3PH_",
        "colab_type": "code",
        "outputId": "2efa60f6-fc2a-4274-e2da-727471d5e8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "val_txt.examples[0].text[0:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " '=',\n",
              " 'homarus',\n",
              " 'gammarus',\n",
              " '=',\n",
              " '<eos>',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " 'homarus',\n",
              " 'gammarus',\n",
              " ',',\n",
              " 'known',\n",
              " 'as',\n",
              " 'the',\n",
              " 'european',\n",
              " 'lobster',\n",
              " 'or']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WU1ySgl101V",
        "colab_type": "code",
        "outputId": "a35603d2-0ae9-4176-8a8b-3a9dfc396753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(val_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDvUtjixs-Hl",
        "colab_type": "text"
      },
      "source": [
        "``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](../_static/img/transformer_input_target.png)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LPIccYUs-Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0sCrJN8s-Hn",
        "colab_type": "text"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuykElVes-Ho",
        "colab_type": "text"
      },
      "source": [
        "The model is set up with the hyperparameter below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPX47qgxs-Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsxlPtt7z4YI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "TransformerEncoder?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAGxnny7s-Hq",
        "colab_type": "text"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCcVcm5Ps-Hr",
        "colab_type": "text"
      },
      "source": [
        "`CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
        "is applied to track the loss and\n",
        "`SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
        "implements stochastic gradient descent method as the optimizer. The initial\n",
        "learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
        "applied to adjust the learn rate through epochs. During the\n",
        "training, we use\n",
        "`nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
        "function to scale all the gradient together to prevent exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wHls-G0s-Hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGGQbUrrfHCD",
        "colab_type": "code",
        "outputId": "5dbfb873-8960-4a53-9935-c17a526baa6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "def run_language_model(eval_model, text):\n",
        "    data = batchify(text, 1)\n",
        "    eval_model.eval() # Turn on the evaluation mode    \n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)       \n",
        "\n",
        "run_language_model(best_model, \"I love cats very much\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-242-13fb84525e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I love cats very much\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-242-13fb84525e33>\u001b[0m in \u001b[0;36mrun_language_model\u001b[0;34m(eval_model, text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0meval_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Turn on the evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-4fLOANs-Ht",
        "colab_type": "text"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlsKPn0ss-Hu",
        "colab_type": "code",
        "outputId": "fd7e85b8-7f7d-4da3-e0e3-a34d095b93d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "    #best_model = model\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 3195 batches | lr 5.00 | ms/batch 38.73 | loss  6.50 | ppl   666.51\n",
            "| epoch   1 |   400/ 3195 batches | lr 5.00 | ms/batch 37.80 | loss  4.41 | ppl    82.55\n",
            "| epoch   1 |   600/ 3195 batches | lr 5.00 | ms/batch 38.83 | loss  3.59 | ppl    36.31\n",
            "| epoch   1 |   800/ 3195 batches | lr 5.00 | ms/batch 37.96 | loss  3.08 | ppl    21.75\n",
            "| epoch   1 |  1000/ 3195 batches | lr 5.00 | ms/batch 38.54 | loss  2.80 | ppl    16.40\n",
            "| epoch   1 |  1200/ 3195 batches | lr 5.00 | ms/batch 38.15 | loss  2.61 | ppl    13.63\n",
            "| epoch   1 |  1400/ 3195 batches | lr 5.00 | ms/batch 38.07 | loss  2.52 | ppl    12.42\n",
            "| epoch   1 |  1600/ 3195 batches | lr 5.00 | ms/batch 38.11 | loss  2.43 | ppl    11.39\n",
            "| epoch   1 |  1800/ 3195 batches | lr 5.00 | ms/batch 38.21 | loss  2.34 | ppl    10.38\n",
            "| epoch   1 |  2000/ 3195 batches | lr 5.00 | ms/batch 38.06 | loss  2.28 | ppl     9.81\n",
            "| epoch   1 |  2200/ 3195 batches | lr 5.00 | ms/batch 38.15 | loss  2.24 | ppl     9.39\n",
            "| epoch   1 |  2400/ 3195 batches | lr 5.00 | ms/batch 38.18 | loss  2.21 | ppl     9.12\n",
            "| epoch   1 |  2600/ 3195 batches | lr 5.00 | ms/batch 38.17 | loss  2.25 | ppl     9.51\n",
            "| epoch   1 |  2800/ 3195 batches | lr 5.00 | ms/batch 39.15 | loss  2.22 | ppl     9.20\n",
            "| epoch   1 |  3000/ 3195 batches | lr 5.00 | ms/batch 38.58 | loss  2.15 | ppl     8.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 127.27s | valid loss  1.17 | valid ppl     3.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 3195 batches | lr 4.75 | ms/batch 38.32 | loss  2.02 | ppl     7.53\n",
            "| epoch   2 |   400/ 3195 batches | lr 4.75 | ms/batch 37.33 | loss  1.96 | ppl     7.10\n",
            "| epoch   2 |   600/ 3195 batches | lr 4.75 | ms/batch 37.30 | loss  1.91 | ppl     6.78\n",
            "| epoch   2 |   800/ 3195 batches | lr 4.75 | ms/batch 37.74 | loss  1.90 | ppl     6.71\n",
            "| epoch   2 |  1000/ 3195 batches | lr 4.75 | ms/batch 38.11 | loss  1.84 | ppl     6.32\n",
            "| epoch   2 |  1200/ 3195 batches | lr 4.75 | ms/batch 38.08 | loss  1.84 | ppl     6.27\n",
            "| epoch   2 |  1400/ 3195 batches | lr 4.75 | ms/batch 38.01 | loss  1.84 | ppl     6.30\n",
            "| epoch   2 |  1600/ 3195 batches | lr 4.75 | ms/batch 38.65 | loss  1.84 | ppl     6.31\n",
            "| epoch   2 |  1800/ 3195 batches | lr 4.75 | ms/batch 38.88 | loss  1.84 | ppl     6.32\n",
            "| epoch   2 |  2000/ 3195 batches | lr 4.75 | ms/batch 38.12 | loss  1.83 | ppl     6.21\n",
            "| epoch   2 |  2200/ 3195 batches | lr 4.75 | ms/batch 39.04 | loss  1.82 | ppl     6.15\n",
            "| epoch   2 |  2400/ 3195 batches | lr 4.75 | ms/batch 37.73 | loss  1.81 | ppl     6.09\n",
            "| epoch   2 |  2600/ 3195 batches | lr 4.75 | ms/batch 37.73 | loss  1.80 | ppl     6.04\n",
            "| epoch   2 |  2800/ 3195 batches | lr 4.75 | ms/batch 39.10 | loss  1.79 | ppl     5.96\n",
            "| epoch   2 |  3000/ 3195 batches | lr 4.75 | ms/batch 39.46 | loss  1.74 | ppl     5.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 127.38s | valid loss  0.94 | valid ppl     2.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 3195 batches | lr 4.51 | ms/batch 39.54 | loss  1.73 | ppl     5.64\n",
            "| epoch   3 |   400/ 3195 batches | lr 4.51 | ms/batch 39.02 | loss  1.69 | ppl     5.43\n",
            "| epoch   3 |   600/ 3195 batches | lr 4.51 | ms/batch 36.99 | loss  1.64 | ppl     5.13\n",
            "| epoch   3 |   800/ 3195 batches | lr 4.51 | ms/batch 39.05 | loss  1.65 | ppl     5.19\n",
            "| epoch   3 |  1000/ 3195 batches | lr 4.51 | ms/batch 37.69 | loss  1.59 | ppl     4.92\n",
            "| epoch   3 |  1200/ 3195 batches | lr 4.51 | ms/batch 38.99 | loss  1.72 | ppl     5.60\n",
            "| epoch   3 |  1400/ 3195 batches | lr 4.51 | ms/batch 38.14 | loss  1.62 | ppl     5.04\n",
            "| epoch   3 |  1600/ 3195 batches | lr 4.51 | ms/batch 37.65 | loss  1.58 | ppl     4.86\n",
            "| epoch   3 |  1800/ 3195 batches | lr 4.51 | ms/batch 39.04 | loss  1.64 | ppl     5.13\n",
            "| epoch   3 |  2000/ 3195 batches | lr 4.51 | ms/batch 38.55 | loss  1.60 | ppl     4.97\n",
            "| epoch   3 |  2200/ 3195 batches | lr 4.51 | ms/batch 38.13 | loss  1.66 | ppl     5.24\n",
            "| epoch   3 |  2400/ 3195 batches | lr 4.51 | ms/batch 37.67 | loss  1.62 | ppl     5.07\n",
            "| epoch   3 |  2600/ 3195 batches | lr 4.51 | ms/batch 38.07 | loss  1.62 | ppl     5.07\n",
            "| epoch   3 |  2800/ 3195 batches | lr 4.51 | ms/batch 37.62 | loss  1.53 | ppl     4.63\n",
            "| epoch   3 |  3000/ 3195 batches | lr 4.51 | ms/batch 38.09 | loss  1.51 | ppl     4.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 127.07s | valid loss  0.89 | valid ppl     2.43\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9bnFeeTOHM3",
        "colab_type": "code",
        "outputId": "f80fae68-9467-4278-c75a-f178513cbbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "best_model.src_mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [-inf, 0., 0.],\n",
              "        [-inf, -inf, 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x4-_CvIs-Hv",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model with the test dataset\n",
        "-------------------------------------\n",
        "\n",
        "Apply the best model to check the result with the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSDSK0fsD48j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtSPfh8Qs-Hw",
        "colab_type": "code",
        "outputId": "e075f45b-53b5-41c8-ff58-91e6a28c8da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  0.87 | test ppl     2.39\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMoOqdKQoskK",
        "colab_type": "code",
        "outputId": "c68d7ca7-7185-4b23-d20d-412fdf4e2c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "p_model = best_model\n",
        "#text = \"robert is an english\n",
        "text = train_txt.examples[0].text[100:135]\n",
        "text = TEXT.preprocess(\"I like\")\n",
        "\n",
        "#text = TEXT.preprocess(\"<sos> I like cats\")\n",
        "print(text)\n",
        "numericed = TEXT.numericalize([text])\n",
        "numericed = numericed.to(device)\n",
        "p_model.eval() # Turn on the evaluation mode    \n",
        "ntokens = len(TEXT.vocab.stoi)\n",
        "with torch.no_grad():            \n",
        "        output = p_model(numericed)  \n",
        "print([TEXT.vocab.itos[i] for i in output.max(2).indices])\n",
        "\n",
        "print(numericed)\n",
        "print(numericed.shape)\n",
        "print(output.max(2).indices)\n",
        "print(output.shape)\n",
        "print(p_model.src_mask)\n",
        "p_model.src_mask.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'like']\n",
            "['like', 'like']\n",
            "tensor([[ 79],\n",
            "        [150]], device='cuda:0')\n",
            "torch.Size([2, 1])\n",
            "tensor([[150],\n",
            "        [150]], device='cuda:0')\n",
            "torch.Size([2, 1, 28871])\n",
            "tensor([[0., 0.],\n",
            "        [-inf, 0.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW6NxGjatEoY",
        "colab_type": "code",
        "outputId": "ae934972-af33-4331-93d3-3441c6e72670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_txt.examples[0].text[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', '<eos>', ' ', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', ' ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zelkij-ntGsK",
        "colab_type": "code",
        "outputId": "2a7b9844-cb2c-41a3-e122-36f38f9db2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "TEXT.numericalize([text])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[640],\n",
              "        [ 29],\n",
              "        [ 36],\n",
              "        [343]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_FnOuQOHAVU",
        "colab_type": "code",
        "outputId": "451f5a14-2433-45ec-bdd6-383d2b6ab2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output[3][0].argmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(262, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czdD6lH3J5uG",
        "colab_type": "code",
        "outputId": "5a834b5b-8f31-4692-da4e-713a1c62ab77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_model.decoder.weight.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28871, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN4mqS7dOc3N",
        "colab_type": "code",
        "outputId": "c41f40c5-826c-41be-8461-c69c936de1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ntokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28871"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ai_EslGO3V7",
        "colab_type": "code",
        "outputId": "231aaac9-dfb1-4c73-c6c8-36800b7d5505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-61353e8b2d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElM1_8hdPCDe",
        "colab_type": "code",
        "outputId": "d4ffebcb-a6a8-4d36-fded-16c15fdefa15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "model.train() # Turn on the train mode\n",
        "total_loss = 0.\n",
        "start_time = time.time()\n",
        "ntokens = len(TEXT.vocab.stoi)\n",
        "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output.view(-1, ntokens), targets)\n",
        "    #print(loss.shape)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    log_interval = 200\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "        cur_loss = total_loss / log_interval\n",
        "        elapsed = time.time() - start_time\n",
        "        print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "              'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "              'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                elapsed * 1000 / log_interval,\n",
        "                cur_loss, math.exp(cur_loss)))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 3195 batches | lr 4.75 | ms/batch 38.64 | loss  1.81 | ppl     6.10\n",
            "| epoch   1 |   400/ 3195 batches | lr 4.75 | ms/batch 37.82 | loss  1.98 | ppl     7.24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-892f77f9385d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(loss.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70UMNICOP7HE",
        "colab_type": "code",
        "outputId": "e355824d-e4fc-4850-d7c8-63118edf39e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tfe8PJDQHM6",
        "colab_type": "code",
        "outputId": "94d78f6e-1742-4267-ceef-d50bb2c52774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "targets.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([700])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qn2xdrQQLBC",
        "colab_type": "code",
        "outputId": "ae322996-d86b-4d47-9b70-4caa219bb5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7QeuvXuQN1y",
        "colab_type": "code",
        "outputId": "3971efc5-77f0-413d-ff9d-27df35fdbd2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13280,   211,  1168,     4,    10,     8,     4, 19981,    36,     4,\n",
              "           26,    10,   218,     5,   207,  9864,    29, 20034,  1130,  2066],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwLOqkjCQibD",
        "colab_type": "code",
        "outputId": "35485c08-c431-4d01-c9ca-f7cb94ea0c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35, 20, 28871])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acYK6zS7Q0tk",
        "colab_type": "code",
        "outputId": "bddd514f-6109-4934-9070-3e63b3e57f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([24504, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzjLuBQSj8Qt",
        "colab_type": "code",
        "outputId": "36175d06-1ee9-4bad-f205-c6879fbf6f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([111832, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-1vODIuj_3T",
        "colab_type": "code",
        "outputId": "53b7096c-c2ce-42f5-b2b8-d666f2dce2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data[0].shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjXm8EQykPR0",
        "colab_type": "code",
        "outputId": "2a3fb372-f821-4a0c-851d-3ee66e111679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
              "            7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjqJOBvgkQtE",
        "colab_type": "code",
        "outputId": "6b1b0950-cd88-4fbb-bcb1-066ec0c09412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([111832, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYaPM-0gmits",
        "colab_type": "code",
        "outputId": "0bd1158c-d36e-4557-af12-ac93e494f9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJgT_0idm2V6",
        "colab_type": "code",
        "outputId": "07625c9f-c405-45a3-a471-ae44b472cfa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "targets.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([700])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYWGIovm37U",
        "colab_type": "code",
        "outputId": "80c87b83-4299-4356-bd69-d7e859794281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "get_batch(train_data, 0)[2].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-4a5d04362023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUMtAV24nc2x",
        "colab_type": "code",
        "outputId": "78f9a586-b8a6-4958-a25c-0eb8db113968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([111832, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTfbXM9Mnlx2",
        "colab_type": "code",
        "outputId": "a1c3bf36-e645-4630-ca20-a7d6a99a6bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.size(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1Sz42YpXuc",
        "colab_type": "code",
        "outputId": "db714b36-c5cc-4d82-aefc-8b22673383df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_model.encoder.weight.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28871, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRG81WT7s9BA",
        "colab_type": "code",
        "outputId": "407c0b2e-32a7-4157-e286-fb43de7a1262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_model.encoder(data).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35, 20, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRUxvtklugf8",
        "colab_type": "code",
        "outputId": "fc73fc1b-e089-4040-b948-5f700797fbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(data.shape)\n",
        "s = best_model.encoder(data) * math.sqrt(best_model.ninp)\n",
        "print(s.shape)\n",
        "s = best_model.pos_encoder(s)\n",
        "print(s.shape)\n",
        "o = best_model.transformer_encoder(s, best_model.src_mask)\n",
        "print(o.shape)\n",
        "o = best_model.decoder(o)\n",
        "print(o.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([35, 20])\n",
            "torch.Size([35, 20, 200])\n",
            "torch.Size([35, 20, 200])\n",
            "torch.Size([35, 20, 200])\n",
            "torch.Size([35, 20, 28871])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy763thau1zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = self.pos_encoder(sample_data)\n",
        "s\n",
        "output = self.transformer_encoder(src, self.src_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wVJDOSHvoDB",
        "colab_type": "code",
        "outputId": "43bd341d-b02a-468f-eac6-9347f3b7641f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_model.src_mask.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35, 35])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIWpwPNivoXP",
        "colab_type": "code",
        "outputId": "f5eb0171-a71b-4069-bb0e-e40507c89aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "best_model.src_mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [-inf, 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpWLqseWvtqg",
        "colab_type": "code",
        "outputId": "6d35cfc5-4f28-41d6-b9ba-3fed41b54fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_model.decoder.weight"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28872, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O30UfjcUwGxu",
        "colab_type": "code",
        "outputId": "2429355f-3d17-4695-aa40-effbb340f569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_model.encoder.weight.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28872, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2vZuIDSB3Fg",
        "colab_type": "code",
        "outputId": "a3f503e6-dd87-4201-f69c-1f3d836d2de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-961ed67232b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lbl5RvS_5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.Linear?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBztHtOpUfHm",
        "colab_type": "code",
        "outputId": "227b512e-edb3-42cb-c763-2600b54ae037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[640],\n",
            "        [  8],\n",
            "        [ 10],\n",
            "        [  9],\n",
            "        [ 29],\n",
            "        [ 36],\n",
            "        [343]], device='cuda:0')\n",
            "torch.Size([7, 1])\n",
            "tensor([[  8],\n",
            "        [ 10],\n",
            "        [  9],\n",
            "        [ 29],\n",
            "        [ 36],\n",
            "        [343],\n",
            "        [343]], device='cuda:0')\n",
            "torch.Size([7, 1, 28872])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxO66aDPgbsK",
        "colab_type": "code",
        "outputId": "f1a89e68-6b9e-49b8-d0f3-b5c8f6120d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.argmax(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(318, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5cfwBB7ZqPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = batchify(\"i like cats\", 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8eOQuIkZr96",
        "colab_type": "code",
        "outputId": "0c3b5450-98f8-42c8-a75f-3d9f222f8363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model(numericed).max(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([[19.5889],\n",
              "        [17.7437],\n",
              "        [17.7850]], device='cuda:0', grad_fn=<MaxBackward0>), indices=tensor([[ 150],\n",
              "        [5992],\n",
              "        [5992]], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeXQXtvlcFpN",
        "colab_type": "code",
        "outputId": "c0232f04-067b-4c08-8392-7895dd5e125b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 28872])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV26QkMeZsiA",
        "colab_type": "code",
        "outputId": "d11a07c8-edfa-41bb-86bd-40c2de3b0e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "train_text.examples[0].text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ea2aa5dfcd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJfu6fRtZ0S8",
        "colab_type": "code",
        "outputId": "9344a932-0f70-46f1-f00a-9d477abd5153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batchify(\"I like cats\".split(' '), 1).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr32Pc_gaPV2",
        "colab_type": "code",
        "outputId": "10a17bfc-b9ed-4b6a-a5a3-f2fd8153139c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2236652, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyjd_r0ZaQYL",
        "colab_type": "code",
        "outputId": "90983702-12b9-4671-9878-99fba98e2951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TEXT.numericalize([\"I\"]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEiPEx6SaoEe",
        "colab_type": "code",
        "outputId": "7f0500c7-cb0d-4320-94ca-4ee5ab018ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "TEXT.numericalize([[\"I\", \"like\", \"cats\"]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0],\n",
              "        [ 150],\n",
              "        [5992]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2KPSD-DaqS3",
        "colab_type": "code",
        "outputId": "bb3bfe59-4212-41c4-e1de-d0c2787c0593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(\"I like cats ver\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f2EdrGTayKv",
        "colab_type": "code",
        "outputId": "2fd96a59-212b-4cdf-9452-6f536c3db249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "values, indices = torch.max(output, 2)\n",
        "values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19.5889],\n",
              "        [17.7437],\n",
              "        [17.7850]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Paj3RL8fci74",
        "colab_type": "code",
        "outputId": "95c49528-a4c5-46b3-85f3-e2412b199b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "indices.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2_b5ENuckxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.max?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND5uOxcmcqBX",
        "colab_type": "code",
        "outputId": "432a371c-a7c9-4812-dd2e-5cfba7c1a4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TEXT.vocab.itos[indices[0]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'like'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGv_exXCdQp3",
        "colab_type": "code",
        "outputId": "0f4c696c-bf16-4428-b518-0fa04965422d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 150],\n",
              "        [5992],\n",
              "        [5992]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918eO6obeKVB",
        "colab_type": "code",
        "outputId": "da4215c1-bde7-46de-a488-2fa91ecdffbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "best_model.src_mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [-inf, 0., 0.],\n",
              "        [-inf, -inf, 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jv2vD4eYV9",
        "colab_type": "code",
        "outputId": "497db478-4572-4ed5-b271-afc42b931ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_txt.examples[0].text[13:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['robert', '<', 'unk', '>', 'is', 'an', 'english']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlSrykbWfQLe",
        "colab_type": "code",
        "outputId": "f6028b6f-8144-4d32-f1b0-aeafaa8c8593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "|y = torch.tensor([\n",
        "     [\n",
        "       [1, 2, 3],\n",
        "       [4, 5, 6]\n",
        "     ],\n",
        "     [\n",
        "       [1, 2, 3],\n",
        "       [4, 5, 6]\n",
        "     ],\n",
        "     [\n",
        "       [1, 2, 3],\n",
        "       [4, 5, 6]\n",
        "     ]\n",
        "   ])\n",
        "y.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P0TOtLFhfb-",
        "colab_type": "code",
        "outputId": "8a1d10b9-a137-4a91-bfb5-102bf2b9d1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.sum(y, dim=0).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAxfTNoWhpoo",
        "colab_type": "code",
        "outputId": "40c11959-374b-4e0a-fd02-77240a00f8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.sum(y, dim=1).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i26jlLJxicAC",
        "colab_type": "code",
        "outputId": "125a7b65-5208-465d-edff-1256c02180a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.sum(y, dim=2).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq0gHQyrideD",
        "colab_type": "code",
        "outputId": "f19ee03c-b125-424b-a3b3-70c214f71505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "TEXT.process(\"I like cats\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
              "        [   0,   14, 1852,   79, 1693, 1638,   14,  656,   15,  201,  565],\n",
              "        [   3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdKlgYPuFSL_",
        "colab_type": "code",
        "outputId": "ff2bdfb2-43cf-4c3a-f774-4fa59002f4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class MyMultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, nhead, dropout):\n",
        "    super(MyMultiHeadAttention, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.nhead = nhead\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.queries_linear = nn.Linear(embed_dim, embed_dim)\n",
        "    self.keys_linear = nn.Linear(embed_dim, embed_dim)\n",
        "    self.values_linear = nn.Linear(embed_dim, embed_dim)\n",
        "    self.final_linear = nn.Linear(embed_dim, embed_dim)\n",
        "    self.head_dim = embed_dim // nhead\n",
        "  def forward(self, sequence_batch, mask):\n",
        "    \"Implements Figure 2\"\n",
        "    #Sequence: Dbatch X Dseq D X embed_dim\n",
        "    queries = self.queries_linear(sequence_batch)\n",
        "    keys = self.queries_linear(sequence_batch)\n",
        "    values = self.values_linear(sequence_batch)\n",
        "    batch_size = sequence_batch.size(0)\n",
        "    queries, keys, values = [linear(sequence_batch) for linear in [self.queries_linear, self.keys_linear, self.values_linear]]\n",
        "    #divide into attention heads:\n",
        "    #Dbatch X nhead X Dseq D X head_dim\n",
        "    queries, keys, values = [attention_part.view(batch_size, -1, self.nhead, self.head_dim).transpose(1,2) for attention_part in [queries, keys, values]]\n",
        "    attended_heads = self.calculatue_attention(queries, keys, values, None)\n",
        "    #concat heads back to \n",
        "    #Dbatch X Dseq D X embed_dim\n",
        "    concated_heads = attended_heads.transpose(1, 2)\\\n",
        "    .contiguous()\\\n",
        "    .view(batch_size, -1, self.embed_dim)\n",
        "    return self.final_linear(concated_heads) # \n",
        "\n",
        "  def calculatue_attention(self, queries, keys, values, mask):\n",
        "    #keys * queries.T * values\n",
        "    #Dbatch X nhead X Dseq X head_dim\n",
        "    weights = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "    #Dbatch X nhead X Dseq D X Dseq D\n",
        "    weights_scaled = weights / math.sqrt(self.head_dim)\n",
        "    if mask is not None:\n",
        "        weights_scaled = weights_scaled.masked_fill(mask == 0, -1e9)\n",
        "    weights_normed = F.softmax(weights_scaled, dim = -1)\n",
        "    if self.dropout:\n",
        "      weights_normed = self.dropout(weights_normed)\n",
        "    attended_keys = torch.matmul(weights_normed, values)\n",
        "    #Dbatch X nhead X Dseq D X head_dim\n",
        "    return attended_keys\n",
        "\n",
        "\n",
        "def test_multi_head_attention():\n",
        "  attention = MyMultiHeadAttention(50, 5, 0.1)\n",
        "  mock_batch = torch.randn((5, 10, 50))\n",
        "  result = attention(mock_batch, None)\n",
        "  print(result.size())\n",
        "test_multi_head_attention()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GELND6sLFiZq",
        "colab_type": "code",
        "outputId": "76c85613-dbcc-489c-b351-bf8db5b5ce70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#weights = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "a = torch.ones((2,2,2,3))\n",
        "b = torch.ones((2,2,3,2))\n",
        "print(a.size)\n",
        "print(torch.matmul(a,b).size())\n",
        "print(torch.matmul(a,b))\n",
        "\n",
        "#torch.matmul(a,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in method size of Tensor object at 0x7fb321bb87e0>\n",
            "torch.Size([2, 2, 2, 2])\n",
            "tensor([[[[3., 3.],\n",
            "          [3., 3.]],\n",
            "\n",
            "         [[3., 3.],\n",
            "          [3., 3.]]],\n",
            "\n",
            "\n",
            "        [[[3., 3.],\n",
            "          [3., 3.]],\n",
            "\n",
            "         [[3., 3.],\n",
            "          [3., 3.]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is1CMDBmwwQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyEncoderLayer(nn.Module):\n",
        "  def __init__(embed_dim, nhead, nhid, dropout):\n",
        "      super(MyEncoderLayer, self).__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.embed_dim = embed_dim\n",
        "      \n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IBaTS0XFRpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93yre14yE8k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.TransformerEncoderLayer??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqIl98QgE_nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.MultiheadAttention??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "733cXHauFpWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn(4, 4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcQKGIv2qTb9",
        "colab_type": "code",
        "outputId": "4cf6a1b5-e91d-43cd-ac3f-8ef234277127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvOEmYyzqVyj",
        "colab_type": "code",
        "outputId": "f0df4296-0323-40e4-e626-e4feaa04a372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ25IoepqZHy",
        "colab_type": "code",
        "outputId": "4685a9e5-f1c6-4377-d457-27779a3fc216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9757,  0.5749, -2.6934,  0.0231],\n",
              "        [-0.7104,  1.5264, -0.3772, -2.3132],\n",
              "        [-1.3506, -0.5612, -0.1005, -0.7492],\n",
              "        [ 1.2988,  0.0405, -0.2952,  1.1514]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgQxNVs5qbhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.matmul??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxV__AHXpN3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}